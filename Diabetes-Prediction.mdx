---
title: 'Practice project on Diabetes Prediction'
image: https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/Diabetes-Prediction/MLP.png
description: This practice project aims to explore predictive analytics using a diabetes dataset, employing two distinct algorithms...
date: '2024-01-17'
tags: ['ai','neuralnetwork','deeplearning','machinelearning','ml','project']
---

This practice project aims to explore predictive analytics using a diabetes dataset, employing two distinct algorithms for comparison – Logistic Regression and Multi-Layer Perceptron (MLP).

Diabetes, a chronic metabolic disorder, poses a substantial global health challenge. Accurate and timely prediction of diabetes onset is crucial for preventive interventions and personalized patient care. In this project, 
we will harness the potential of machine learning to develop predictive models capable of identifying patterns within a diabetes dataset. The choice of Logistic Regression and MLP as the algorithms for comparison is motivated by their distinct methodologies, 
allowing us to evaluate their respective strengths and weaknesses in handling complex medical data.

Throughout this project, we will outline the preprocessing steps, feature engineering, and model training processes for both Logistic Regression and MLP. 
The comparative analysis will not only shed light on their predictive capabilities but also offer insights into the interpretability and generalizability of each model. 
The ultimate goal is to equip practitioners and enthusiasts with a deeper understanding of the nuances involved in selecting the appropriate machine learning algorithm for diabetes prediction tasks.

- **Interpretability**: Logistic Regression is generally more interpretable than complex models like MLP. The coefficients in Logistic Regression directly correspond to the impact of each feature on the output, 
providing a clear understanding of the model's decision process. MLP, being a neural network with multiple layers, can be more challenging to interpret due to its intricate architecture.\
Interpretability helps you see the impact of each piece of information on the model's output. For instance, if a model predicts loan approval, interpretability shows you how factors like income, credit score, and debt influence the decision.

- **Generalizability**: The generalizability of the models will be assessed based on their performance on unseen data. A model that can generalize well to new diabetes cases beyond the training dataset is preferable, 
as it indicates the ability to capture underlying patterns common to different instances of the problem. The comparison between Logistic Regression and MLP will help evaluate which model is more effective in terms of generalization for the given diabetes dataset.


# Diabetes Prediction

Let’s start writing practice project in jupyter notebook.
I ma using  [pima-indians-diabetes-database](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)


```
pip install -r requirements.txt
```
This command reads the requirements.txt file and installs all the specified dependencies.

Make sure you have Python and pip installed on your system, and the pip executable is added to your system's PATH.

```Python
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy import stats

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_recall_curve, auc, precision_score, recall_score, roc_curve, roc_auc_score

import tensorflow as tf
```

Reading dataset from the local file.
```Python
diabetes_DS = pd.read_csv('data/diabetes.csv')
```

Now, Displays the first few rows of the dataset, statistical summary, shape (number of rows and columns), and the count of different classes in the **Outcome** column (0 - Non-Diabetic, 1 - Diabetic).

```Python
diabetes_DS.head()
```
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/Diabetes-Prediction/dshead.png"  alt=""  width={941} height={234} />

```Python
diabetes_DS.describe()
```
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/Diabetes-Prediction/dsdescribe.png"  alt=""  width={1000} height={348} />

```Python
diabetes_DS.shape
```
(768, 9)

```Python
diabetes_DS['Outcome'].value_counts()
```
Outcome \
0    500 \
1    268

0 - Non-Diabetic \
1 - Diabetic


The provided code uses the groupby function to group a DataFrame named diabetes_DS by the 'Outcome' column, calculates the mean for each group, and transposes the result. 
The resulting DataFrame df contains columns for 'Outcome' 0 and 1, with rows representing mean values for each original column in the dataset.
```Python
df = diabetes_DS.groupby('Outcome').mean().T
df
```
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/Diabetes-Prediction/groupbyt.png"  alt=""  width={336} height={275} />

This formula calculates the percentage difference between values in columns labeled 0 and 1 in the DataFrame 'df'. The result is then stored in a new column labeled '%diff'. This formula is commonly used to measure the relative difference between two values.
```Python
df['%diff'] = (abs(df[0] - df[1] )/ ((df[0] + df[1])/2)) *100
df
```
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/Diabetes-Prediction/groupbytd.png"  alt=""  width={410} height={275} />

Dropping the column named 'Outcome' from the DataFrame diabetes_DS.
```Python
data = diabetes_DS.drop(columns= 'Outcome', axis=1)
data.head()
```

```Python
data.hist(bins=30, figsize=(15,10))
plt.show()
```
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/Diabetes-Prediction/hist.png"  alt=""  width={1000} height={834} />

Computes the correlation matrix for the features in the dataset and visualizes it using a heatmap. This helps to identify relationships between different features.
```Python
corr_matrix = data.corr()
corr_matrix
```
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/Diabetes-Prediction/corr.png"  alt=""  width={900} height={272} />

A correlation coefficient of 0.54, representing the maximum correlation between the variables "Pregnancies" and "Age," is considered moderate. This moderate correlation does not raise concerns.
```Python
sns.heatmap(corr_matrix, annot= True)
```
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/Diabetes-Prediction/corrheat.png"  alt=""  width={663} height={568} />

## Preprocessing
performing imputation for missing values in specific columns of the diabetes dataset using the median value of each respective column.

```Python
# Calculate the median value for BMI
median_bmi = diabetes_DS['BMI'].median()
# Substitute it in the BMI column of the
# dataset where values are 0
diabetes_DS['BMI'] = diabetes_DS['BMI'].replace(
    to_replace=np.nan, value=median_bmi)

median_bloodp = diabetes_DS['BloodPressure'].median()
# Substitute it in the BloodP column of the
# dataset where values are 0
diabetes_DS['BloodPressure'] = diabetes_DS['BloodPressure'].replace(
    to_replace=np.nan, value=median_bloodp)

# Calculate the median value for PlGlcConc
median_plglcconc = diabetes_DS['Glucose'].median()
# Substitute it in the PlGlcConc column of the
# dataset where values are 0
diabetes_DS['Glucose'] = diabetes_DS['Glucose'].replace(
    to_replace=np.nan, value=median_plglcconc)

# Calculate the median value for SkinThick
median_skinthick = diabetes_DS['SkinThickness'].median()
# Substitute it in the SkinThick column of the
# dataset where values are 0
diabetes_DS['SkinThickness'] = diabetes_DS['SkinThickness'].replace(
    to_replace=np.nan, value=median_skinthick)

# Calculate the median value for SkinThick
median_skinthick = diabetes_DS['Insulin'].median()
# Substitute it in the SkinThick column of the
# dataset where values are 0
diabetes_DS['Insulin'] = diabetes_DS['Insulin'].replace(
    to_replace=np.nan, value=median_skinthick)
```

**Use Z-scores to identify and filter out the outliers.**
The boxen plot for Insulin shows that the data is skewed, with a long tail of high values. This means that there are a few people with very high Insulin levels, which could be due to outliers or a different population.

```Python
diabetes_DS.boxplot(figsize=(15,10))
```

<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/Diabetes-Prediction/boxplotb.png"  alt=""  width={1000} height={800} />

- $z = \frac{{X - \mu}}{{\sigma}}$
    - $Z$ is the z-score, a standardized value that tells you how many standard deviations a particular data point  $(X)$  is from the mean  $(\mu)$
    - $X$ is the raw score or data point you're interested in.
    - $\mu$ is the mean of the data set, which is the average value.
    - ${\sigma}$ is the standard deviation, a measure of how spread out the values in a data set are.
- If the Z-score is positive, the data point is above the mean.
- If the Z-score is negative, the data point is below the mean.
- A Z-score of 0 indicates that the data point's value is exactly at the mean.
- By setting a threshold for Z-scores, you can identify and potentially remove data points that deviate significantly from the mean, helping to detect outliers in the dataset. 
The threshold value of 3 is a common choice, considering that data points beyond three standard deviations from the mean are considered unusual in a normal distribution. Adjusting the threshold allows for more or fewer data points to be flagged as outliers.
- Applies the .all(axis=1) operation along axis 1, which corresponds to the columns in the boolean array. This operation checks if all conditions are True for each row. 
The result is a boolean mask indicating whether all Z-scores for each data point are below the threshold.

```Python
# Define a threshold for the Z-score
z_threshold = 3
# Calculate the Z-scores for each column
z_scores = stats.zscore(diabetes_DS)

# Create a boolean mask for outliers
outlier_mask = (abs(z_scores) < z_threshold).all(axis=1)

# Filter the dataset to remove outliers
diabetes_DS2 = diabetes_DS[outlier_mask]

# Print information about the removed outliers
outliers_removed = diabetes_DS.shape[0] - diabetes_DS2.shape[0]
print(f"Number of outliers removed: {outliers_removed}")
```
Number of outliers removed: 80

**After z**

<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/Diabetes-Prediction/boxplota.png"  alt=""  width={1000} height={800} />
