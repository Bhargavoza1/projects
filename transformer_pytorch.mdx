---
title: 'Implementing Transformer Models in PyTorch: A Guided Walkthrough'
image: https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/transformer/title.png
description: In recent years, transformer models have revolutionized the field of natural language processing (NLP) and have found...
date: '2024-06-05'
tags: ['transformer','ai','neuralnetwork','deeplearning','nlp','language_translator','project']
---

In recent years, transformer models have revolutionized the field of natural language processing (NLP) and have found applications in various other domains such as computer vision and time series forecasting. Their ability to handle long-range dependencies and parallelize training has made them the go-to architecture for many state-of-the-art models.

Nowadays, transformers and their variants are everywhere. Let's deep dive into it and understand its code from scratch. In this article, we will explore the implementation of transformer models in PyTorch, leveraging the excellent tutorial and GitHub repository by Umar Jamil.

We will follow along with Umar Jamil's comprehensive [YouTube](https://www.youtube.com/watch?v=ISNdQcPhsts) tutorial and reference his [GitHub](https://github.com/hkproj/pytorch-transformer) repository to understand the intricate details of transformer models. This article is designed for those who already have a solid foundation in machine learning and PyTorch and are looking to expand their knowledge by delving into advanced models.

While utilizing these resources, I am also creating my own repository on GitHub to update it to the latest version and incorporate improvements, especially taking advantage of my RTX 4070 Ti GPU for efficient training and experimentation.

# Prerequisite Knowledge

1. Python
2. PyTorch
3. Fundamental of artificial neural networks

In case you are unfamiliar with artificial neural networks, please feel free to explore my blog by clicking on the provided link. [Artificial Neural Network](https://bhargavoza.com/blogs/Artificial%20Neural%20Network)
You can access my repository by using the provided link: [transformer](https://github.com/Bhargavoza1/transformer)


# How to run this?

```
pip install -r requiremnets.txt
```
and run train_test.py


# Transformers Architecture
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/transformer/tf_ar.png"  alt=""  width={768} height={1000} />
Don't worry about the diagram, I'll break down everything step by step and explain each component and block in this transformer model architecture.

Understanding Transformer Models in Simple Terms

1. **Attention**: Imagine you're reading a story. Some words are more important for understanding than others. Attention in a transformer is like giving more focus to the important words while reading.

2. **Encoder**: Think of this as the part of the model that understands the story you're reading. It figures out the meaning of each word and how they relate to each other.

3. **Decoder**: This part of the model uses what the encoder understood to create a new story, maybe in a different language. It's like translating or summarizing.

4. **Positional Encoding**: Since the model doesn't naturally know the order of words, positional encoding is like giving each word a number to show its place in the story.

5. **Word Embedding**: When the model reads words, it doesn't see them like we do. Instead, it turns each word into a special code called a vector. These vectors help the model understand the meaning of words based on how they're used in the story.

6. **Feed-Forward Neural Networks (FFNN)**: These are like super smart calculators. They help the model understand complex patterns in the story.

7. **Residual Connections**: Imagine if you're building a tall tower with blocks. Sometimes, to make sure the tower doesn't fall, you add extra support. Residual connections are like that extra support for the model, helping it learn better.

8. **Layer Normalization**: This is like making sure each part of the model is using the same scale or rules to understand the story. It keeps everything fair and balanced.

9. **Multi-Head Attention**: Think of this as having multiple pairs of eyes reading the story at the same time, each looking for different important parts.

10. **Masking**: Just like you wouldn't peek ahead in a book to spoil the ending, masking ensures the model focuses only on the parts of the story it has already "read." It's like covering up the pages ahead, so the model can't cheat by looking into the future while learning.

# Deep Dive into Transformers with code

In this deep dive, we will explore the Transformer model, focusing on a practical use case: translating text from English to Italian.

In this guide, we'll break down the process into several stages, starting with data preprocessing. 

## Data Preprocessing

**Tokenization:** Is important for transformer models in natural language processing because it breaks down text into smaller parts, called tokens, that the model can understand. This helps the model learn patterns in the data and handle different input lengths and unknown words, improving its ability to understand and generate human language.

To put it simply, tokenization assigns each word a specific index, like a unique key in a database. These indexed words are then passed into the transformer model, which uses them to perform complex mathematical calculations.

train_test.py
```Python
def get_or_build_tokenizer(config, ds, lang):
    # Construct the file path for the tokenizer using the language-specific format
    tokenizer_path = Path(config['tokenizer_file'].format(lang))

    # Check if the tokenizer file exists at the specified path
    if not Path.exists(tokenizer_path):
        # If the tokenizer file does not exist, create a new WordLevel tokenizer
        tokenizer = Tokenizer(WordLevel(unk_token="[UNK]"))

        # Set the pre-tokenizer to split text by whitespace
        tokenizer.pre_tokenizer = Whitespace()

        # Define a trainer for the tokenizer with special tokens and a minimum frequency threshold
        trainer = WordLevelTrainer(special_tokens=["[UNK]", "[PAD]", "[SOS]", "[EOS]"], min_frequency=2)

        # Train the tokenizer on sentences from the dataset for the specified language
        # 'get_all_sentences(ds, lang)' is return an iterator over all sentences
        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)

        # Save the trained tokenizer to the specified file path
        tokenizer.save(str(tokenizer_path))
    else:
        # If the tokenizer file exists, load the tokenizer from the file
        tokenizer = Tokenizer.from_file(str(tokenizer_path))

    # Return the tokenizer, either newly created or loaded from the file
    return tokenizer
```

<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/transformer/tokan.png"  alt=""  width={768} height={768} />

torch.utils.data.Dataset is a powerful tool in PyTorch for creating custom datasets. By defining how to access and retrieve data, you can handle complex data pipelines and integrate seamlessly with PyTorch's data loading utilities.
"Dataset" is an abstract class, meaning you don't use it directly. Instead, you create a subclass and implement specific methods to define how data should be accessed and manipulated. 
In the code provided below, we have implemented our own custom logic to include an additional token in our sentences.

Let's skip the causal_mask function for now. I'll cover what masking is in more detail later on in our walk through.

dataset.py
```Python
import torch
from torch.utils.data import Dataset

class BilingualDataset(Dataset):

    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):
        super().__init__()
        self.seq_len = seq_len

        self.ds = ds
        self.tokenizer_src = tokenizer_src
        self.tokenizer_tgt = tokenizer_tgt
        self.src_lang = src_lang
        self.tgt_lang = tgt_lang

        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id("[SOS]")], dtype=torch.int64)
        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id("[EOS]")], dtype=torch.int64)
        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id("[PAD]")], dtype=torch.int64)

    def __len__(self):
        return len(self.ds)

    def __getitem__(self, idx):
        src_target_pair = self.ds[idx]
        src_text = src_target_pair['translation'][self.src_lang]
        tgt_text = src_target_pair['translation'][self.tgt_lang]

        # Transform the text into tokens
        enc_input_tokens = self.tokenizer_src.encode(src_text).ids
        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids

        # Add sos, eos and padding to each sentence
        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2  # We will add <s> and </s>
        # We will only add <s>, and </s> only on the label
        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1

        # Make sure the number of padding tokens is not negative. If it is, the sentence is too long
        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:
            raise ValueError("Sentence is too long")

        # Add <s> and </s> token
        encoder_input = torch.cat(
            [
                self.sos_token,
                torch.tensor(enc_input_tokens, dtype=torch.int64),
                self.eos_token,
                torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype=torch.int64),
            ],
            dim=0,
        )

        # Add only <s> token
        decoder_input = torch.cat(
            [
                self.sos_token,
                torch.tensor(dec_input_tokens, dtype=torch.int64),
                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),
            ],
            dim=0,
        )

        # Add only </s> token
        label = torch.cat(
            [
                torch.tensor(dec_input_tokens, dtype=torch.int64),
                self.eos_token,
                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype=torch.int64),
            ],
            dim=0,
        )

        # Double check the size of the tensors to make sure they are all seq_len long
        assert encoder_input.size(0) == self.seq_len
        assert decoder_input.size(0) == self.seq_len
        assert label.size(0) == self.seq_len

        return {
            "encoder_input": encoder_input,  # (seq_len)
            "decoder_input": decoder_input,  # (seq_len)
            "encoder_mask": (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(), # (1, 1, seq_len)
            "decoder_mask": (decoder_input != self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), # (1, seq_len) & (1, seq_len, seq_len),
            "label": label,  # (seq_len)
            "src_text": src_text,
            "tgt_text": tgt_text,
        }
    
def causal_mask(size):
    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)
    return mask == 0


```

Train test split

train_test.py
```Python
def get_ds(config):
    # Load the raw dataset based on the datasource and language pair from the configuration
    # The dataset only has the train split, so we divide it ourselves into training and validation sets
    ds_raw = load_dataset(f"{config['datasource']}", f"{config['lang_src']}-{config['lang_tgt']}", split='train')

    # Build or retrieve tokenizers for both source and target languages using the raw dataset
    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config['lang_src'])
    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config['lang_tgt'])

    # Calculate the sizes for training and validation datasets (90% for training, 10% for validation)
    train_ds_size = int(0.9 * len(ds_raw))
    val_ds_size = len(ds_raw) - train_ds_size
    
    # Randomly split the raw dataset into training and validation sets based on the calculated sizes
    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])

    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'],
                                config['seq_len'])
    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'],
                              config['seq_len'])

    # Find the maximum length of each sentence in the source and target sentence
    #region to chacke max len. other then that we do not have any use of this block
    max_len_src = 0
    max_len_tgt = 0

    for item in ds_raw:
        src_ids = tokenizer_src.encode(item['translation'][config['lang_src']]).ids
        tgt_ids = tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids
        max_len_src = max(max_len_src, len(src_ids))
        max_len_tgt = max(max_len_tgt, len(tgt_ids))

    print(f'Max length of source sentence: {max_len_src}')
    print(f'Max length of target sentence: {max_len_tgt}')
    #endregion

    train_dataloader = DataLoader(train_ds, batch_size=config['batch_size'], shuffle=True)
    val_dataloader = DataLoader(val_ds, batch_size=1, shuffle=True)

    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt
```

## Word Embedding
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/transformer/word_em.png"  alt=""  width={768} height={768} />
Word embeddings encode semantic information about words. Words with similar meanings are represented by vectors that are closer together in the embedding space. This allows the model to understand the relationships between words and capture semantic similarities.

Word embeddings transform words or tokens into dense numerical vectors in a continuous vector space. Each word in a vocabulary is represented by a unique vector, typically of fixed length.

Here we are using a dimension of 512.

model.py
```Python
class InputEmbeddings(nn.Module):

    def __init__(self, d_model: int, vocab_size: int) -> None:
        super().__init__()
        self.d_model = d_model
        self.vocab_size = vocab_size
        self.embedding = nn.Embedding(vocab_size, d_model)

    def forward(self, x):
        # (batch, seq_len) --> (batch, seq_len, d_model)
        # Multiply by sqrt(d_model) to scale the embeddings according to the paper
        return self.embedding(x) * math.sqrt(self.d_model)
```

Over time, the performance of finding semantic similarity of words will improve as the transformer model is trained.

## Positional Encoding
Transformers process input tokens in parallel rather than sequentially (unlike RNNs), they need a way to encode the position of each token in the sequence.
Positional Encoding provides a way to inject some information about the position of each token within the sequence into the model. This helps the transformer to differentiate between different positions and understand the order of the tokens. 

Positional Encodings are added to the input embeddings at the bottom of the encoder and decoder stacks. There are several ways to implement positional encoding, but the most common method used in the original transformer paper "Attention is All You Need" employs sinusoidal functions of different frequencies. 

**Sine Component**
```math
\text{PE}_{(pos, 2i)} = \sin \left( \frac{\text{pos}}{10000^{\frac{2i}{d_{model}}}} \right) 
```
**Cosine Component**
```math
\text{PE}_{(pos, 2i+1)} = \cos \left( \frac{\text{pos}}{10000^{\frac{2i}{d_{model}}}} \right) 
```
### Key Reasons for Using Multiple Frequencies

1. **Encoding Different Scales of Relationships:**
   * Different frequency components enable the model to capture both short-term and long-term dependencies.
   * Higher frequencies correspond to fine-grained (local) positional differences, whereas lower frequencies capture broader (global) positional patterns.

2. **Uniqueness of Position Representation:**
   * Using a combination of sine and cosine functions at different frequencies ensures that each position has a unique encoding. This uniqueness helps the model distinguish between different positions effectively.
   * If we used a single frequency, the positional encodings might not be distinct enough to represent different positions clearly.

3. **Periodicity and Continuity:**
   * The sine and cosine functions are periodic, which means they repeat their values in regular intervals. This periodicity can help the model understand cyclical patterns in the data.
   * The continuous nature of these functions allows for smooth interpolation between positions, which can be beneficial when dealing with sequences of varying lengths.

model.py
```Python
class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:
        super().__init__()
        self.d_model = d_model
        self.seq_len = seq_len
        self.dropout = nn.Dropout(dropout)
        # Create a matrix of shape (seq_len, d_model)
        pe = torch.zeros(seq_len, d_model)
        # Create a vector of shape (seq_len)
        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)
        # Create a vector of shape (d_model)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) # (d_model / 2)
        # Apply sine to even indices
        pe[:, 0::2] = torch.sin(position * div_term) # sin(position * (10000 ** (2i / d_model))
        # Apply cosine to odd indices
        pe[:, 1::2] = torch.cos(position * div_term) # cos(position * (10000 ** (2i / d_model))
        # Add a batch dimension to the positional encoding
        pe = pe.unsqueeze(0) # (1, seq_len, d_model)
        # Register the positional encoding as a buffer
        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)
        return self.dropout(x)
```

## Multi-Head Attention
"Attention is All You Need," revolutionized the field of natural language processing (NLP) by leveraging attention mechanisms to process sequential data. 
Central to the Transformer’s success is the multi-head attention mechanism, which allows the model to attend to different parts of a sequence simultaneously, 
capturing complex relationships and dependencies more effectively than previous architectures like recurrent neural networks (RNNs) or convolutional neural networks (CNNs). 
In this section, we will delve into the intricacies of the multi-head attention mechanism, explaining its components and functionality step by step.

<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/transformer/mha.png"  alt=""  width={1024} height={512} />

### Linear Projections

**Queries (Q)** : Queries are representations of the input sequence that the model uses to ask questions about the importance of different tokens. For each token in the sequence, a query vector is generated to interact with the keys of all tokens. The query essentially determines "which parts of the input sequence should be attended to" when processing a particular token.

**Keys (K)** : Keys are another set of vectors derived from the input sequence, representing the tokens in a way that makes them comparable with the queries. Each key vector can be thought of as an answer to a potential query. When a query from one token interacts with the keys of all tokens, it determines how much focus or attention each token should receive relative to the query token.

**Values (V)** : Values are the representations of the input sequence that are used to produce the final output of the attention mechanism. While queries and keys determine the attention weights, values are the actual content that gets combined to form the output. The values are weighted by the attention scores to reflect the importance of each token as determined by the interaction of queries and keys.

For self-attention, the same input embeddings are used to generate queries (Q), keys (K), and values (V) through learned linear transformations. Each token’s embedding is projected into these three distinct spaces:

$$Q=XW_Q$$ \
$$K=XW_K$$ \
$$V=XW_V$$ 


Where $$W_Q$$, $$W_K$$ ,and $$W_V$$ are learned weight matrices of dimensions $$dxd_k$$. Consequently, Q, K, and V are matrices of size $$nxd_k$$.

### Scaled Dot-Product Attention

Each attention head computes the attention scores using the scaled dot-product attention mechanism. This involves three main steps:

**Compute Dot Products :** The dot products between the query vectors and key vectors are computed to get raw attention scores: 

scores = $$QK^T$$

**Scale the Scores :** The raw attention scores are scaled by the square root of the dimension of the key vectors $$d_k$$. This helps in stabilizing the gradients during training:

scaled_scores= $$\frac{scores}{\sqrt{d_k}}$$

**Apply Softmax :**  The scaled scores are passed through a softmax function to obtain the attention weights. This normalizes the scores to a probability distribution:

attention_weights = softmax(scaled_scores)

**Weighted Sum of Values :** The attention weights are used to compute a weighted sum of the value vectors, producing the output for each attention head:

head_output = attention_weights ⋅ V

### Multi-Head Attention 
While a single attention mechanism is powerful, the Transformer employs multiple attention heads to capture various aspects of the relationships in the data. Each head operates independently, and their outputs are concatenated:

concat_output=Concat(head1​,head2​,...,headh​)

### Final Linear Projection
The concatenated output from all the heads is then linearly projected back to the original embedding dimension d using another learned weight matrix $$W_O$$ :

multi_head_output=concat_output$$W_O$$

model.py
```Python
class MultiHeadAttentionBlock(nn.Module):

    def __init__(self, d_model: int, h: int, dropout: float) -> None:
        super().__init__()
        self.d_model = d_model # Embedding vector size
        self.h = h # Number of heads
        # Make sure d_model is divisible by h
        assert d_model % h == 0, "d_model is not divisible by h"

        self.d_k = d_model // h # Dimension of vector seen by each head
        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq
        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk
        self.w_v = nn.Linear(d_model, d_model, bias=False) # Wv
        self.w_o = nn.Linear(d_model, d_model, bias=False) # Wo
        self.dropout = nn.Dropout(dropout)

    @staticmethod
    def attention(query, key, value, mask, dropout: nn.Dropout):
        d_k = query.shape[-1]
        # Just apply the formula from the paper
        # (batch, h, seq_len, d_k) --> (batch, h, seq_len, seq_len)
        attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)
        if mask is not None:
            # Write a very low value (indicating -inf) to the positions where mask == 0
            attention_scores.masked_fill_(mask == 0, -1e9)
        attention_scores = attention_scores.softmax(dim=-1) # (batch, h, seq_len, seq_len) # Apply softmax
        if dropout is not None:
            attention_scores = dropout(attention_scores)
        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)
        # return attention scores which can be used for visualization
        return (attention_scores @ value), attention_scores

    def forward(self, q, k, v, mask):
        query = self.w_q(q) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)
        key = self.w_k(k) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)
        value = self.w_v(v) # (batch, seq_len, d_model) --> (batch, seq_len, d_model)

        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)
        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)
        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)
        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)

        # Calculate attention
        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)
        
        # Combine all the heads together
        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)
        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)

        # Multiply by Wo
        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)  
        return self.w_o(x)
```



