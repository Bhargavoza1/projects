---
title: 'Implementing Transformer Models in PyTorch: A Guided Walkthrough'
image: https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/transformer/title.png
description: cutting-edge kidney tumor classification system! This project represents the culmination of my efforts...
date: '2024-06-05'
tags: ['transformer','ai','neuralnetwork','deeplearning','nlp','language_translator','project']
---

In recent years, transformer models have revolutionized the field of natural language processing (NLP) and have found applications in various other domains such as computer vision and time series forecasting. Their ability to handle long-range dependencies and parallelize training has made them the go-to architecture for many state-of-the-art models.

Nowadays, transformers and their variants are everywhere. Let's deep dive into it and understand its code from scratch. In this article, we will explore the implementation of transformer models in PyTorch, leveraging the excellent tutorial and GitHub repository by Umar Jamil.

We will follow along with Umar Jamil's comprehensive [YouTube](https://www.youtube.com/watch?v=ISNdQcPhsts) tutorial and reference his [GitHub](https://github.com/hkproj/pytorch-transformer) repository to understand the intricate details of transformer models. This article is designed for those who already have a solid foundation in machine learning and PyTorch and are looking to expand their knowledge by delving into advanced models.

While utilizing these resources, I am also creating my own repository on GitHub to update it to the latest version and incorporate improvements, especially taking advantage of my RTX 4070 Ti GPU for efficient training and experimentation.

# Prerequisite Knowledge

1. Python
2. PyTorch
3. Fundamental of artificial neural networks

In case you are unfamiliar with artificial neural networks, please feel free to explore my blog by clicking on the provided link. [Artificial Neural Network](https://bhargavoza.com/blogs/Artificial%20Neural%20Network)
You can access my repository by using the provided link: [transformer](https://github.com/Bhargavoza1/transformer)


# How to run this?

```
pip install -r requiremnets.txt
```
and run train_test.py


# Transformers Architecture
<CustomImage src="https://raw.githubusercontent.com/Bhargavoza1/projects/main/images/transformer/tf_ar.png"  alt=""  width={768} height={1000} />
Don't worry about the diagram, I'll break down everything step by step and explain each component and block in this transformer model architecture.

Understanding Transformer Models in Simple Terms

1. **Attention**: Imagine you're reading a story. Some words are more important for understanding than others. Attention in a transformer is like giving more focus to the important words while reading.

2. **Encoder**: Think of this as the part of the model that understands the story you're reading. It figures out the meaning of each word and how they relate to each other.

3. **Decoder**: This part of the model uses what the encoder understood to create a new story, maybe in a different language. It's like translating or summarizing.

4. **Positional Encoding**: Since the model doesn't naturally know the order of words, positional encoding is like giving each word a number to show its place in the story.

5. **Word Embedding**: When the model reads words, it doesn't see them like we do. Instead, it turns each word into a special code called a vector. These vectors help the model understand the meaning of words based on how they're used in the story.

6. **Feed-Forward Neural Networks (FFNN)**: These are like super smart calculators. They help the model understand complex patterns in the story.

7. **Residual Connections**: Imagine if you're building a tall tower with blocks. Sometimes, to make sure the tower doesn't fall, you add extra support. Residual connections are like that extra support for the model, helping it learn better.

8. **Layer Normalization**: This is like making sure each part of the model is using the same scale or rules to understand the story. It keeps everything fair and balanced.

9. **Multi-Head Attention**: Think of this as having multiple pairs of eyes reading the story at the same time, each looking for different important parts.

10. **Masking**: Just like you wouldn't peek ahead in a book to spoil the ending, masking ensures the model focuses only on the parts of the story it has already "read." It's like covering up the pages ahead, so the model can't cheat by looking into the future while learning.

# Deep Dive into Transformers with code

In this deep dive, we will explore the Transformer model, focusing on a practical use case: translating text from English to Italian.

In this guide, we'll break down the process into several stages, starting with data preprocessing. 

## Data Preprocessing

**Tokenization:** Is important for transformer models in natural language processing because it breaks down text into smaller parts, called tokens, that the model can understand. This helps the model learn patterns in the data and handle different input lengths and unknown words, improving its ability to understand and generate human language.

To put it in simpler terms, we are giving each word a specific index.

train_test.py
```Python
def get_or_build_tokenizer(config, ds, lang):
    # Construct the file path for the tokenizer using the language-specific format
    tokenizer_path = Path(config['tokenizer_file'].format(lang))

    # Check if the tokenizer file exists at the specified path
    if not Path.exists(tokenizer_path):
        # If the tokenizer file does not exist, create a new WordLevel tokenizer
        tokenizer = Tokenizer(WordLevel(unk_token="[UNK]"))

        # Set the pre-tokenizer to split text by whitespace
        tokenizer.pre_tokenizer = Whitespace()

        # Define a trainer for the tokenizer with special tokens and a minimum frequency threshold
        trainer = WordLevelTrainer(special_tokens=["[UNK]", "[PAD]", "[SOS]", "[EOS]"], min_frequency=2)

        # Train the tokenizer on sentences from the dataset for the specified language
        # 'get_all_sentences(ds, lang)' is return an iterator over all sentences
        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)

        # Save the trained tokenizer to the specified file path
        tokenizer.save(str(tokenizer_path))
    else:
        # If the tokenizer file exists, load the tokenizer from the file
        tokenizer = Tokenizer.from_file(str(tokenizer_path))

    # Return the tokenizer, either newly created or loaded from the file
    return tokenizer
```
